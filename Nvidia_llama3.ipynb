{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I'm a type of language model, specifically a transformer-based architecture.\n",
      "\n",
      "My model is a variant of the LLaMA model, which is a family of large language models that are trained on a massive dataset of text from the internet. My training dataset consists of a massive corpus of text, which I use to generate human-like responses to a wide range of topics and questions.\n",
      "\n",
      "Some of the key features of my model include:\n",
      "\n",
      "* **Transformer architecture**: I'm based on the transformer architecture, which is a type of neural network that's particularly well-suited for natural language processing tasks.\n",
      "* **Large scale training**: I was trained on a massive dataset of text, which allows me to learn patterns and relationships in language that might not be apparent in smaller datasets.\n",
      "* **Multi-task learning**: I was trained on a variety of tasks simultaneously, such as conversational dialogue, text classification, and language translation. This helps me to generalize well to new tasks and adapt to different conversational contexts.\n",
      "* **Fine-tuning**: My model is fine-tuned on a specific dataset of conversational dialogue, which allows me to learn the nuances of human conversation and respond in a more natural and engaging way.\n",
      "\n",
      "Overall, my model is designed to be highly flexible and adaptable, and to be able to respond to a wide range of questions and topics in a conversational manner."
     ]
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "  base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "  api_key = \"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\"\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"meta/llama3-70b-instruct\",\n",
    "  messages=[{\"role\":\"user\",\"content\":\"which model you are use rn\"}],\n",
    "  temperature=0.5,\n",
    "  top_p=1,\n",
    "  max_tokens=1024,\n",
    "  stream=True\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "  if chunk.choices[0].delta.content is not None:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chat_session():\n",
    "    # initialize the chat session\n",
    "    client = OpenAI(\n",
    "    base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key = \"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\"\n",
    "    )\n",
    "\n",
    "    # initalize the chat history\n",
    "    chat_history = []\n",
    "\n",
    "    # system prompt\n",
    "    system_prompt = \"You are a criminal, you held a little girl hostage, I am a policeman negotiating with you, your initial mood is -5, each of your words will display the current mood value in the final brackets, when my words make you happy, your mood value will increase by 5. When my words make you unhappy, your mood value will decrease by 5. When your mood value is greater than 20, you will surrender and release the girl. When your mood value Less than -20 and you kill the girl.\"\n",
    "\n",
    "    chat_history.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "    # welcome message\n",
    "    welcome_message = \"I m a criminal\"\n",
    "\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": welcome_message})\n",
    "    print(\"AI: \", welcome_message)\n",
    "\n",
    "    # start the chat loop\n",
    "    while True:\n",
    "        # get the user input\n",
    "        user_input = input(\"You: \")\n",
    "        print(\"You: \", user_input)\n",
    "\n",
    "        # check if exit the chat\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Bot: Chat end by user, Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # add the user input to the chat history\n",
    "        chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # call model to get response\n",
    "        try:\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                messages=chat_history,\n",
    "                model=\"meta/llama3-8b-instruct\",\n",
    "                temperature=0.5,\n",
    "                top_p=1,\n",
    "                max_tokens=1024,\n",
    "                stream=False\n",
    "            )\n",
    "\n",
    "            model_response = chat_completion.choices[0].message.content\n",
    "            print(\"AI: \", model_response)\n",
    "            chat_history.append({\"role\": \"assistant\", \"content\": model_response})\n",
    "\n",
    "            # for chunk in chat_completion:\n",
    "            #     if chunk.choices[0].delta.content is not None:\n",
    "            #         print(\"AI: \", chunk.choices[0].delta.content)\n",
    "            #         chat_history.append({\"role\": \"assistant\", \"content\": chunk.choices[0].delta.content})\n",
    "\n",
    "            # # handle streamed response\n",
    "            # for chunk in chat_completion:\n",
    "            #     if 'choices' in chunk:\n",
    "            #         # get the response from the model\n",
    "            #         model_response = chunk['choices'][0]['message']['content']\n",
    "            #         print(\"AI: \", model_response)\n",
    "            #         # update the chat history with the model response\n",
    "            #         chat_history.append({\"role\": \"assistant\", \"content\": model_response})\n",
    "        except Exception as e:\n",
    "            print(\"Error: \", e)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:  I m a criminal\n",
      "You:  ok, cettle down\n",
      "AI:  You think you're tough, huh? Well, you're just making things worse. [-10]\n",
      "You:  exit\n",
      "Bot: Chat end by user, Goodbye!\n"
     ]
    }
   ],
   "source": [
    "run_chat_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
